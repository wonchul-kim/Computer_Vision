# Bootstrap Your Own Latent： A New Approach to Self-Supervised Learning 

```
Some self-supervised methods are not contrastive but rely on using auxiliary handcrafted prediction tasks to learn their representation. In particular, relative patch prediction [23, 40], colorizing gray-scale images [41, 42], image
inpainting [43], image jigsaw puzzle [44], image super-resolution [45], and geometric transformations [46, 47] have been shown to be useful. Yet, even with suitable architectures [48], these methods are being outperformed by contrastive methods [37, 8, 12].
```

기존의 self-supervised learning의 한 기법인 pretext task보다 성능이 좋다는 것을 본 논문에서 보여주고 있다.


## 